{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:/百度/train+cnn.txt', encoding='utf8') as file:\n",
    "    line_list1 = [k.strip() for k in file.readlines()]\n",
    "    #读取每行\n",
    "    train_label_list = [k.split()[0] for k in line_list1]\n",
    "    #将标签依次取出\n",
    "    train_content_list = [k.split(maxsplit=1)[1] for k in line_list1]\n",
    "    #将内容依次取出,此处注意split()选择最大分割次数为1,否则句子被打断.\n",
    "with open('E:/百度/test+cnn.txt', encoding='utf8') as file:\n",
    "    line_list2 = [k.strip() for k in file.readlines()]\n",
    "    test_label_list = [k.split()[0] for k in line_list2]\n",
    "    test_content_list = [k.split(maxsplit=1)[1] for k in line_list2]\n",
    "with open('E:/百度/val+cnn.txt', encoding='utf8') as file:\n",
    "    line_list3 = [k.strip() for k in file.readlines()]\n",
    "    val_label_list = [k.split()[0] for k in line_list3]\n",
    "    val_content_list = [k.split(maxsplit=1)[1] for k in line_list3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\hkdw235\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "from keras.layers import Embedding\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec,LineSentence,Text8Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('cont.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128 # 词向量空间维度\n",
    "num_classes=10 # 类型数量\n",
    "maxlen=600 # 文本长度\n",
    "max_words=10000 # 词汇表数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "for word, vocab_obj in model.wv.vocab.items():\n",
    "    if int(vocab_obj.index) < max_words:\n",
    "        embeddings_index[word] = word_vectors[word]\n",
    "del model, word_vectors # 删掉gensim模型释放内存\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 383707 unique tokens2.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words) # 传入我们词向量的字典\n",
    "content_list=train_content_list+test_content_list+val_content_list\n",
    "tokenizer.fit_on_texts(content_list) # 传入我们的训练数据，得到训练数据中出现的词的字典\n",
    "sequences1 = tokenizer.texts_to_sequences(train_content_list) # 根据训练数据中出现的词的字典，将训练数据转换为sequences\n",
    "sequences2 = tokenizer.texts_to_sequences(test_content_list) # 根据训练数据中出现的词的字典，将训练数据转换为sequences\n",
    "sequences3 = tokenizer.texts_to_sequences(val_content_list) # 根据训练数据中出现的词的字典，将训练数据转换为sequences\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens2.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label=LabelEncoder()\n",
    "train_Y = to_categorical(label.fit_transform(train_label_list),num_classes=num_classes)\n",
    "test_Y = to_categorical(label.fit_transform(test_label_list),num_classes=num_classes)\n",
    "val_Y = to_categorical(label.fit_transform(val_label_list),num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pad_sequences(sequences1,maxlen=maxlen,truncating='post',padding='post') # 和原作者的不同\n",
    "test_X = pad_sequences(sequences2,maxlen=maxlen,truncating='post',padding='post') # 和原作者不同 \n",
    "val_X = pad_sequences(sequences3,maxlen=maxlen,truncating='post',padding='post') # 和原作者不同 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((max_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # 文本数据中的词在词向量字典中没有，向量为取0；如果有则取词向量中该词的向量\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 600, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 593, 256)          262400    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,577,098\n",
      "Trainable params: 296,842\n",
      "Non-trainable params: 1,280,256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# 将预训练好的词向量加载如embedding layer\n",
    "# 我们设置 trainable = False，代表词向量不作为参数进行更新\n",
    "embedding_layer = model.add(layers.Embedding(max_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=False))\n",
    "con=model.add(layers.Conv1D(filters=256,kernel_size=8))\n",
    "maxpool=model.add(layers.MaxPooling1D(593))\n",
    "flatten=model.add(layers.Flatten())\n",
    "dense1=model.add(layers.Dense(128))  # 128是输出层的维度\n",
    "dropout=model.add(layers.Dropout(0.2))  # 控制需要断开的神经元比例，此处应该为0.2\n",
    "bn=model.add(layers.normalization.BatchNormalization(axis=1))  # 规范层加速\n",
    "active1=model.add(layers.core.Activation('relu'))  # 激活函数为relu\n",
    "dense2=model.add(layers.Dense(10)) #最后的输出层\n",
    "predict_y=model.add(layers.core.Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "optimizer = optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['categorical_accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:100 loss:0.2306 accuracy:0.9200\n",
      "step:200 loss:0.1763 accuracy:0.9300\n",
      "step:300 loss:0.1097 accuracy:0.9600\n",
      "step:400 loss:0.1031 accuracy:0.9700\n",
      "step:500 loss:0.0786 accuracy:0.9800\n",
      "step:600 loss:0.0945 accuracy:0.9700\n",
      "step:700 loss:0.0637 accuracy:0.9800\n",
      "step:800 loss:0.1440 accuracy:0.9500\n",
      "step:900 loss:0.0485 accuracy:0.9800\n",
      "step:1000 loss:0.0525 accuracy:0.9800\n",
      "step:1100 loss:0.2993 accuracy:0.8800\n",
      "step:1200 loss:0.1810 accuracy:0.9100\n",
      "step:1300 loss:0.6527 accuracy:0.8000\n",
      "step:1400 loss:0.0792 accuracy:0.9700\n",
      "step:1500 loss:0.2569 accuracy:0.9400\n",
      "step:1600 loss:0.2446 accuracy:0.9200\n",
      "step:1700 loss:0.0918 accuracy:0.9700\n",
      "step:1800 loss:0.0419 accuracy:0.9900\n",
      "step:1900 loss:0.1250 accuracy:0.9800\n",
      "step:2000 loss:0.1434 accuracy:0.9600\n",
      "step:2100 loss:0.2688 accuracy:0.9200\n",
      "step:2200 loss:0.0993 accuracy:0.9800\n",
      "step:2300 loss:0.0320 accuracy:0.9900\n",
      "step:2400 loss:0.0575 accuracy:0.9800\n",
      "step:2500 loss:0.1194 accuracy:0.9600\n",
      "step:2600 loss:0.0796 accuracy:0.9700\n",
      "step:2700 loss:0.2312 accuracy:0.9400\n",
      "step:2800 loss:0.5177 accuracy:0.8900\n",
      "step:2900 loss:0.3040 accuracy:0.9400\n",
      "step:3000 loss:0.2692 accuracy:0.9000\n",
      "step:3100 loss:0.1824 accuracy:0.9600\n",
      "step:3200 loss:0.1170 accuracy:0.9700\n",
      "step:3300 loss:0.1861 accuracy:0.9100\n",
      "step:3400 loss:0.0778 accuracy:0.9700\n",
      "step:3500 loss:0.1224 accuracy:0.9600\n",
      "step:3600 loss:0.1738 accuracy:0.9500\n",
      "step:3700 loss:0.1824 accuracy:0.9500\n",
      "step:3800 loss:0.1308 accuracy:0.9600\n",
      "step:3900 loss:0.1852 accuracy:0.9500\n",
      "step:4000 loss:0.0805 accuracy:0.9700\n",
      "step:4100 loss:0.1810 accuracy:0.9500\n",
      "step:4200 loss:0.0595 accuracy:0.9600\n",
      "step:4300 loss:0.2393 accuracy:0.9300\n",
      "step:4400 loss:0.0945 accuracy:0.9800\n",
      "step:4500 loss:0.1281 accuracy:0.9600\n",
      "step:4600 loss:0.0732 accuracy:0.9500\n",
      "step:4700 loss:0.0534 accuracy:0.9800\n",
      "step:4800 loss:0.2957 accuracy:0.9400\n",
      "step:4900 loss:0.0274 accuracy:0.9900\n",
      "step:5000 loss:0.0350 accuracy:0.9800\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5000):\n",
    "    train_index = random.sample(list(range(len(train_Y))),k=64)\n",
    "    X = train_X[train_index]\n",
    "    Y = train_Y[train_index]\n",
    "    model.fit(X,Y,verbose=0)\n",
    "    step = i + 1 \n",
    "    if step % 100 == 0:\n",
    "        val_index = random.sample(list(range(len(val_Y))), k=100)\n",
    "        x = val_X[val_index]\n",
    "        y = val_Y[val_index]\n",
    "        loss_value, accuracy_value = model.evaluate(x,y,verbose=0)\n",
    "        print('step:%d loss:%.4f accuracy:%.4f' %(step, loss_value, accuracy_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import mglearn\n",
    "import matplotlib.pyplot as plt\n",
    "def predictAll(test_X, batch_size=100):\n",
    "    predict_value_list = []\n",
    "    for i in range(0, len(test_X), batch_size):\n",
    "        X = test_X[i: i + batch_size]\n",
    "        predict_value = model.predict(X)\n",
    "        predict_value_list.extend(predict_value)\n",
    "    return np.array(predict_value_list)\n",
    "\n",
    "Y = predictAll(test_X)\n",
    "y = np.argmax(Y, axis=1)\n",
    "predict_label_list = label.inverse_transform(y)\n",
    "score_image = mglearn.tools.heatmap(confusion_matrix(test_label_list, predict_label_list),\n",
    "                                    xlabel='Predicted label',ylabel='True label',xticklabels=label.classes_,\n",
    "                                    yticklabels=label.classes_,cmap=plt.cm.gray_r,fmt='%d')\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.title('混淆矩阵')\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>体育</td>\n",
       "      <td>0.996016</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.998004</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>0.996917</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.983274</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>家居</td>\n",
       "      <td>0.990453</td>\n",
       "      <td>0.8300</td>\n",
       "      <td>0.903156</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>房产</td>\n",
       "      <td>0.960976</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.972840</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>教育</td>\n",
       "      <td>0.967675</td>\n",
       "      <td>0.9280</td>\n",
       "      <td>0.947422</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>时尚</td>\n",
       "      <td>0.913761</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.953110</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>时政</td>\n",
       "      <td>0.931262</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.959263</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>游戏</td>\n",
       "      <td>0.955727</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.974007</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>科技</td>\n",
       "      <td>0.992813</td>\n",
       "      <td>0.9670</td>\n",
       "      <td>0.979737</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>财经</td>\n",
       "      <td>0.954633</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.971513</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>总体</td>\n",
       "      <td>0.966023</td>\n",
       "      <td>0.9647</td>\n",
       "      <td>0.964232</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Label  Precision  Recall        F1  Support\n",
       "0      体育   0.996016  1.0000  0.998004     1000\n",
       "1      娱乐   0.996917  0.9700  0.983274     1000\n",
       "2      家居   0.990453  0.8300  0.903156     1000\n",
       "3      房产   0.960976  0.9850  0.972840     1000\n",
       "4      教育   0.967675  0.9280  0.947422     1000\n",
       "5      时尚   0.913761  0.9960  0.953110     1000\n",
       "6      时政   0.931262  0.9890  0.959263     1000\n",
       "7      游戏   0.955727  0.9930  0.974007     1000\n",
       "8      科技   0.992813  0.9670  0.979737     1000\n",
       "9      财经   0.954633  0.9890  0.971513     1000\n",
       "999    总体   0.966023  0.9647  0.964232    10000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def eval_model(y_true, y_pred, labels):\n",
    "    # 计算每个分类的Precision, Recall, f1, support\n",
    "    p, r, f1, s = precision_recall_fscore_support(y_true, y_pred)\n",
    "    # 计算总体的平均Precision, Recall, f1, support\n",
    "    tot_p = np.average(p, weights=s)\n",
    "    tot_r = np.average(r, weights=s)\n",
    "    tot_f1 = np.average(f1, weights=s)\n",
    "    tot_s = np.sum(s)\n",
    "    res1 = pd.DataFrame({\n",
    "        u'Label': labels,\n",
    "        u'Precision': p,\n",
    "        u'Recall': r,\n",
    "        u'F1': f1,\n",
    "        u'Support': s\n",
    "    })\n",
    "    res2 = pd.DataFrame({\n",
    "        u'Label': ['总体'],\n",
    "        u'Precision': [tot_p],\n",
    "        u'Recall': [tot_r],\n",
    "        u'F1': [tot_f1],\n",
    "        u'Support': [tot_s]\n",
    "    })\n",
    "    res2.index = [999]\n",
    "    res = pd.concat([res1, res2])\n",
    "    return res[['Label', 'Precision', 'Recall', 'F1', 'Support']]\n",
    "\n",
    "eval_model(test_label_list, predict_label_list, label.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
