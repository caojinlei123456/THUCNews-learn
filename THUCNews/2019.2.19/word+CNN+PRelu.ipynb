{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:/百度/train+cnn.txt', encoding='utf8') as file:\n",
    "    line_list1 = [k.strip() for k in file.readlines()]\n",
    "    #读取每行\n",
    "    train_label_list = [k.split()[0] for k in line_list1]\n",
    "    #将标签依次取出\n",
    "    train_content_list = [k.split(maxsplit=1)[1] for k in line_list1]\n",
    "    #将内容依次取出,此处注意split()选择最大分割次数为1,否则句子被打断.\n",
    "with open('E:/百度/test+cnn.txt', encoding='utf8') as file:\n",
    "    line_list2 = [k.strip() for k in file.readlines()]\n",
    "    test_label_list = [k.split()[0] for k in line_list2]\n",
    "    test_content_list = [k.split(maxsplit=1)[1] for k in line_list2]\n",
    "with open('E:/百度/val+cnn.txt', encoding='utf8') as file:\n",
    "    line_list3 = [k.strip() for k in file.readlines()]\n",
    "    val_label_list = [k.split()[0] for k in line_list3]\n",
    "    val_content_list = [k.split(maxsplit=1)[1] for k in line_list3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\hkdw235\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "from keras.layers import Embedding\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec,LineSentence,Text8Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('cont.model')\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128 # 词向量空间维度\n",
    "num_classes=10 # 类型数量\n",
    "maxlen=600 # 文本长度\n",
    "max_words=10000 # 词汇表数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "for word, vocab_obj in model.wv.vocab.items():\n",
    "    if int(vocab_obj.index) < max_words:\n",
    "        embeddings_index[word] = word_vectors[word]\n",
    "del model, word_vectors # 删掉gensim模型释放内存\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 383707 unique tokens2.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words) # 传入我们词向量的字典\n",
    "content_list=train_content_list+test_content_list+val_content_list\n",
    "tokenizer.fit_on_texts(content_list) # 传入我们的训练数据，得到训练数据中出现的词的字典\n",
    "sequences1 = tokenizer.texts_to_sequences(train_content_list) # 根据训练数据中出现的词的字典，将训练数据转换为sequences\n",
    "sequences2 = tokenizer.texts_to_sequences(test_content_list) # 根据训练数据中出现的词的字典，将训练数据转换为sequences\n",
    "sequences3 = tokenizer.texts_to_sequences(val_content_list) # 根据训练数据中出现的词的字典，将训练数据转换为sequences\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens2.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label=LabelEncoder()\n",
    "train_Y = to_categorical(label.fit_transform(train_label_list),num_classes=num_classes)\n",
    "test_Y = to_categorical(label.fit_transform(test_label_list),num_classes=num_classes)\n",
    "val_Y = to_categorical(label.fit_transform(val_label_list),num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pad_sequences(sequences1,maxlen=maxlen,truncating='post',padding='post') # 和原作者的不同\n",
    "test_X = pad_sequences(sequences2,maxlen=maxlen,truncating='post',padding='post') # 和原作者不同 \n",
    "val_X = pad_sequences(sequences3,maxlen=maxlen,truncating='post',padding='post') # 和原作者不同 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from keras import layers\n",
    "from keras.layers import advanced_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((max_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # 文本数据中的词在词向量字典中没有，向量为取0；如果有则取词向量中该词的向量\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 600, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 593, 256)          262400    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "p_re_lu_1 (PReLU)            (None, 128)               128       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,576,714\n",
      "Trainable params: 296,714\n",
      "Non-trainable params: 1,280,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# 将预训练好的词向量加载如embedding layer\n",
    "# 我们设置 trainable = False，代表词向量不作为参数进行更新\n",
    "embedding_layer = model.add(layers.Embedding(max_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=False))\n",
    "con=model.add(layers.Conv1D(filters=256,kernel_size=8))\n",
    "maxpool=model.add(layers.MaxPooling1D(593))\n",
    "flatten=model.add(layers.Flatten())\n",
    "dense1=model.add(layers.Dense(128, input_dim=100))  # 64是输出层的维度\n",
    "dropout=model.add(layers.Dropout(0.2))  # 控制需要断开的神经元比例，此处应该为0.2\n",
    "active1=model.add(advanced_activations.PReLU()) # 使用PRELU激活函数\n",
    "dense2=model.add(layers.Dense(10, input_shape=(600,128)))\n",
    "predict_y=model.add(layers.core.Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "optimizer = optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['categorical_accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:100 loss:0.3125 accuracy:0.9100\n",
      "step:200 loss:0.1675 accuracy:0.9500\n",
      "step:300 loss:0.1263 accuracy:0.9600\n",
      "step:400 loss:0.1001 accuracy:0.9700\n",
      "step:500 loss:0.1436 accuracy:0.9600\n",
      "step:600 loss:0.1419 accuracy:0.9800\n",
      "step:700 loss:0.1177 accuracy:0.9500\n",
      "step:800 loss:0.1727 accuracy:0.9400\n",
      "step:900 loss:0.1010 accuracy:0.9700\n",
      "step:1000 loss:0.0924 accuracy:0.9600\n",
      "step:1100 loss:0.0938 accuracy:0.9600\n",
      "step:1200 loss:0.2521 accuracy:0.9500\n",
      "step:1300 loss:0.1504 accuracy:0.9300\n",
      "step:1400 loss:0.0312 accuracy:0.9900\n",
      "step:1500 loss:0.0587 accuracy:0.9800\n",
      "step:1600 loss:0.1747 accuracy:0.9500\n",
      "step:1700 loss:0.1545 accuracy:0.9800\n",
      "step:1800 loss:0.0943 accuracy:0.9900\n",
      "step:1900 loss:0.1430 accuracy:0.9600\n",
      "step:2000 loss:0.1461 accuracy:0.9600\n",
      "step:2100 loss:0.3813 accuracy:0.9200\n",
      "step:2200 loss:0.0661 accuracy:0.9700\n",
      "step:2300 loss:0.4006 accuracy:0.9100\n",
      "step:2400 loss:0.2485 accuracy:0.9500\n",
      "step:2500 loss:0.1624 accuracy:0.9700\n",
      "step:2600 loss:0.3460 accuracy:0.9400\n",
      "step:2700 loss:0.0888 accuracy:0.9700\n",
      "step:2800 loss:0.0502 accuracy:0.9700\n",
      "step:2900 loss:0.0792 accuracy:0.9900\n",
      "step:3000 loss:0.3079 accuracy:0.9200\n",
      "step:3100 loss:0.0543 accuracy:0.9800\n",
      "step:3200 loss:0.0949 accuracy:0.9700\n",
      "step:3300 loss:0.0274 accuracy:0.9900\n",
      "step:3400 loss:0.2843 accuracy:0.9600\n",
      "step:3500 loss:0.0417 accuracy:0.9800\n",
      "step:3600 loss:0.0837 accuracy:0.9800\n",
      "step:3700 loss:0.2985 accuracy:0.8900\n",
      "step:3800 loss:0.0002 accuracy:1.0000\n",
      "step:3900 loss:0.0814 accuracy:0.9800\n",
      "step:4000 loss:0.2268 accuracy:0.9500\n",
      "step:4100 loss:0.1404 accuracy:0.9600\n",
      "step:4200 loss:0.1731 accuracy:0.9600\n",
      "step:4300 loss:0.1177 accuracy:0.9400\n",
      "step:4400 loss:0.1896 accuracy:0.9300\n",
      "step:4500 loss:0.1504 accuracy:0.9500\n",
      "step:4600 loss:0.1131 accuracy:0.9600\n",
      "step:4700 loss:0.2614 accuracy:0.9700\n",
      "step:4800 loss:0.1197 accuracy:0.9700\n",
      "step:4900 loss:0.3152 accuracy:0.9400\n",
      "step:5000 loss:0.1515 accuracy:0.9700\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5000):\n",
    "    train_index = random.sample(list(range(len(train_Y))),k=64)\n",
    "    X = train_X[train_index]\n",
    "    Y = train_Y[train_index]\n",
    "    model.fit(X,Y,verbose=0)\n",
    "    step = i + 1 \n",
    "    if step % 100 == 0:\n",
    "        val_index = random.sample(list(range(len(val_Y))), k=100)\n",
    "        x = val_X[val_index]\n",
    "        y = val_Y[val_index]\n",
    "        loss_value, accuracy_value = model.evaluate(x,y,verbose=0)\n",
    "        print('step:%d loss:%.4f accuracy:%.4f' %(step, loss_value, accuracy_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import mglearn\n",
    "import matplotlib.pyplot as plt\n",
    "def predictAll(test_X, batch_size=100):\n",
    "    predict_value_list = []\n",
    "    for i in range(0, len(test_X), batch_size):\n",
    "        X = test_X[i: i + batch_size]\n",
    "        predict_value = model.predict(X)\n",
    "        predict_value_list.extend(predict_value)\n",
    "    return np.array(predict_value_list)\n",
    "\n",
    "Y = predictAll(test_X)\n",
    "y = np.argmax(Y, axis=1)\n",
    "predict_label_list = label.inverse_transform(y)\n",
    "score_image = mglearn.tools.heatmap(confusion_matrix(test_label_list, predict_label_list),\n",
    "                                    xlabel='Predicted label',ylabel='True label',xticklabels=label.classes_,\n",
    "                                    yticklabels=label.classes_,cmap=plt.cm.gray_r,fmt='%d')\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.title('混淆矩阵')\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>体育</td>\n",
       "      <td>0.996997</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.996498</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>0.988024</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>家居</td>\n",
       "      <td>0.976344</td>\n",
       "      <td>0.9080</td>\n",
       "      <td>0.940933</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>房产</td>\n",
       "      <td>0.978109</td>\n",
       "      <td>0.9830</td>\n",
       "      <td>0.980549</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>教育</td>\n",
       "      <td>0.983535</td>\n",
       "      <td>0.8960</td>\n",
       "      <td>0.937729</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>时尚</td>\n",
       "      <td>0.959302</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>0.974409</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>时政</td>\n",
       "      <td>0.947471</td>\n",
       "      <td>0.9740</td>\n",
       "      <td>0.960552</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>游戏</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>0.968215</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>科技</td>\n",
       "      <td>0.951830</td>\n",
       "      <td>0.9880</td>\n",
       "      <td>0.969578</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>财经</td>\n",
       "      <td>0.978218</td>\n",
       "      <td>0.9880</td>\n",
       "      <td>0.983085</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>总体</td>\n",
       "      <td>0.970720</td>\n",
       "      <td>0.9703</td>\n",
       "      <td>0.970056</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Label  Precision  Recall        F1  Support\n",
       "0      体育   0.996997  0.9960  0.996498     1000\n",
       "1      娱乐   0.988024  0.9900  0.989011     1000\n",
       "2      家居   0.976344  0.9080  0.940933     1000\n",
       "3      房产   0.978109  0.9830  0.980549     1000\n",
       "4      教育   0.983535  0.8960  0.937729     1000\n",
       "5      时尚   0.959302  0.9900  0.974409     1000\n",
       "6      时政   0.947471  0.9740  0.960552     1000\n",
       "7      游戏   0.947368  0.9900  0.968215     1000\n",
       "8      科技   0.951830  0.9880  0.969578     1000\n",
       "9      财经   0.978218  0.9880  0.983085     1000\n",
       "999    总体   0.970720  0.9703  0.970056    10000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def eval_model(y_true, y_pred, labels):\n",
    "    # 计算每个分类的Precision, Recall, f1, support\n",
    "    p, r, f1, s = precision_recall_fscore_support(y_true, y_pred)\n",
    "    # 计算总体的平均Precision, Recall, f1, support\n",
    "    tot_p = np.average(p, weights=s)\n",
    "    tot_r = np.average(r, weights=s)\n",
    "    tot_f1 = np.average(f1, weights=s)\n",
    "    tot_s = np.sum(s)\n",
    "    res1 = pd.DataFrame({\n",
    "        u'Label': labels,\n",
    "        u'Precision': p,\n",
    "        u'Recall': r,\n",
    "        u'F1': f1,\n",
    "        u'Support': s\n",
    "    })\n",
    "    res2 = pd.DataFrame({\n",
    "        u'Label': ['总体'],\n",
    "        u'Precision': [tot_p],\n",
    "        u'Recall': [tot_r],\n",
    "        u'F1': [tot_f1],\n",
    "        u'Support': [tot_s]\n",
    "    })\n",
    "    res2.index = [999]\n",
    "    res = pd.concat([res1, res2])\n",
    "    return res[['Label', 'Precision', 'Recall', 'F1', 'Support']]\n",
    "\n",
    "eval_model(test_label_list, predict_label_list, label.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
