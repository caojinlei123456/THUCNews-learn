{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:/百度/train+cnn.txt', encoding='utf8') as file:\n",
    "    line_list1 = [k.strip() for k in file.readlines()]\n",
    "    #读取每行\n",
    "    train_label_list = [k.split()[0] for k in line_list1]\n",
    "    #将标签依次取出\n",
    "    train_content_list = [k.split(maxsplit=1)[1] for k in line_list1]\n",
    "    #将内容依次取出,此处注意split()选择最大分割次数为1,否则句子被打断.\n",
    "with open('E:/百度/test+cnn.txt', encoding='utf8') as file:\n",
    "    line_list2 = [k.strip() for k in file.readlines()]\n",
    "    test_label_list = [k.split()[0] for k in line_list2]\n",
    "    test_content_list = [k.split(maxsplit=1)[1] for k in line_list2]\n",
    "with open('E:/百度/val+cnn.txt', encoding='utf8') as file:\n",
    "    line_list3 = [k.strip() for k in file.readlines()]\n",
    "    val_label_list = [k.split()[0] for k in line_list3]\n",
    "    val_content_list = [k.split(maxsplit=1)[1] for k in line_list3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\hkdw235\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "from keras.layers import Embedding\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec,LineSentence,Text8Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('C:/Users/hkdw235/PycharmProjects/untitled/THUCNews/cont.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128 # 词向量空间维度\n",
    "num_classes=10 # 类型数量\n",
    "maxlen=600 # 文本长度\n",
    "max_words=10000 # 词汇表数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "for word, vocab_obj in model.wv.vocab.items():\n",
    "    if int(vocab_obj.index) < max_words:\n",
    "        embeddings_index[word] = word_vectors[word]\n",
    "del model, word_vectors # 删掉gensim模型释放内存\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 383707 unique tokens2.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words) # 传入我们词向量的字典\n",
    "content_list=train_content_list+test_content_list+val_content_list\n",
    "tokenizer.fit_on_texts(content_list) # 传入我们的训练数据，得到训练数据中出现的词的字典\n",
    "sequences1 = tokenizer.texts_to_sequences(train_content_list) # 根据训练数据中出现的词的字典，将训练数据转换为sequences\n",
    "sequences2 = tokenizer.texts_to_sequences(test_content_list) # 根据训练数据中出现的词的字典，将训练数据转换为sequences\n",
    "sequences3 = tokenizer.texts_to_sequences(val_content_list) # 根据训练数据中出现的词的字典，将训练数据转换为sequences\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens2.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label=LabelEncoder()\n",
    "train_Y = to_categorical(label.fit_transform(train_label_list),num_classes=num_classes)\n",
    "test_Y = to_categorical(label.fit_transform(test_label_list),num_classes=num_classes)\n",
    "val_Y = to_categorical(label.fit_transform(val_label_list),num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pad_sequences(sequences1,maxlen=maxlen,truncating='post',padding='post') # 和原作者的不同\n",
    "test_X = pad_sequences(sequences2,maxlen=maxlen,truncating='post',padding='post') # 和原作者不同 \n",
    "val_X = pad_sequences(sequences3,maxlen=maxlen,truncating='post',padding='post') # 和原作者不同 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from keras import layers\n",
    "from keras.initializers import he_normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((max_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # 文本数据中的词在词向量字典中没有，向量为取0；如果有则取词向量中该词的向量\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 600, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 593, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,420,106\n",
      "Trainable params: 140,106\n",
      "Non-trainable params: 1,280,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# 将预训练好的词向量加载如embedding layer\n",
    "# 我们设置 trainable = False，代表词向量不作为参数进行更新\n",
    "embedding_layer = model.add(layers.Embedding(max_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=False))\n",
    "con=model.add(layers.Conv1D(filters=128,kernel_size=8))\n",
    "maxpool=model.add(layers.MaxPooling1D(593))\n",
    "flatten=model.add(layers.Flatten())\n",
    "dense1=model.add(layers.Dense(64,activation='relu',kernel_initializer='glorot_normal'))  # 64是输出层的维度\n",
    "dropout=model.add(layers.Dropout(0.2))  # 控制需要断开的神经元比例，此处应该为0.2\n",
    "dense2=model.add(layers.Dense(10))\n",
    "predict_y=model.add(layers.core.Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "optimizer = optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['categorical_accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:100 loss:0.1280 accuracy:0.9700\n",
      "step:200 loss:0.1123 accuracy:0.9700\n",
      "step:300 loss:0.1507 accuracy:0.9600\n",
      "step:400 loss:0.1773 accuracy:0.9300\n",
      "step:500 loss:0.1031 accuracy:0.9700\n",
      "step:600 loss:0.2073 accuracy:0.9500\n",
      "step:700 loss:0.2654 accuracy:0.9500\n",
      "step:800 loss:0.1727 accuracy:0.9700\n",
      "step:900 loss:0.1825 accuracy:0.9400\n",
      "step:1000 loss:0.1721 accuracy:0.9700\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(1000):\n",
    "    train_index = random.sample(list(range(len(train_Y))),k=200)\n",
    "    X = train_X[train_index]\n",
    "    Y = train_Y[train_index]\n",
    "    model.fit(X,Y,verbose=0)\n",
    "    step = i + 1 \n",
    "    if step % 100 == 0:\n",
    "        val_index = random.sample(list(range(len(val_Y))), k=100)\n",
    "        x = val_X[val_index]\n",
    "        y = val_Y[val_index]\n",
    "        loss_value, accuracy_value = model.evaluate(x,y,verbose=0)\n",
    "        print('step:%d loss:%.4f accuracy:%.4f' %(step, loss_value, accuracy_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import mglearn\n",
    "import matplotlib.pyplot as plt\n",
    "def predictAll(test_X, batch_size=100):\n",
    "    predict_value_list = []\n",
    "    for i in range(0, len(test_X), batch_size):\n",
    "        X = test_X[i: i + batch_size]\n",
    "        predict_value = model.predict(X)\n",
    "        predict_value_list.extend(predict_value)\n",
    "    return np.array(predict_value_list)\n",
    "\n",
    "Y = predictAll(test_X)\n",
    "y = np.argmax(Y, axis=1)\n",
    "predict_label_list = label.inverse_transform(y)\n",
    "score_image = mglearn.tools.heatmap(confusion_matrix(test_label_list, predict_label_list),\n",
    "                                    xlabel='Predicted label',ylabel='True label',xticklabels=label.classes_,\n",
    "                                    yticklabels=label.classes_,cmap=plt.cm.gray_r,fmt='%d')\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.title('混淆矩阵')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>体育</td>\n",
       "      <td>0.997994</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.996495</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>0.988012</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>家居</td>\n",
       "      <td>0.972885</td>\n",
       "      <td>0.8970</td>\n",
       "      <td>0.933403</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>房产</td>\n",
       "      <td>0.981763</td>\n",
       "      <td>0.9690</td>\n",
       "      <td>0.975340</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>教育</td>\n",
       "      <td>0.984530</td>\n",
       "      <td>0.8910</td>\n",
       "      <td>0.935433</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>时尚</td>\n",
       "      <td>0.915741</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.950962</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>时政</td>\n",
       "      <td>0.954102</td>\n",
       "      <td>0.9770</td>\n",
       "      <td>0.965415</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>游戏</td>\n",
       "      <td>0.965753</td>\n",
       "      <td>0.9870</td>\n",
       "      <td>0.976261</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>科技</td>\n",
       "      <td>0.958824</td>\n",
       "      <td>0.9780</td>\n",
       "      <td>0.968317</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>财经</td>\n",
       "      <td>0.955854</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.975514</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>总体</td>\n",
       "      <td>0.967546</td>\n",
       "      <td>0.9668</td>\n",
       "      <td>0.966564</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Label  Precision  Recall        F1  Support\n",
       "0      体育   0.997994  0.9950  0.996495     1000\n",
       "1      娱乐   0.988012  0.9890  0.988506     1000\n",
       "2      家居   0.972885  0.8970  0.933403     1000\n",
       "3      房产   0.981763  0.9690  0.975340     1000\n",
       "4      教育   0.984530  0.8910  0.935433     1000\n",
       "5      时尚   0.915741  0.9890  0.950962     1000\n",
       "6      时政   0.954102  0.9770  0.965415     1000\n",
       "7      游戏   0.965753  0.9870  0.976261     1000\n",
       "8      科技   0.958824  0.9780  0.968317     1000\n",
       "9      财经   0.955854  0.9960  0.975514     1000\n",
       "999    总体   0.967546  0.9668  0.966564    10000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def eval_model(y_true, y_pred, labels):\n",
    "    # 计算每个分类的Precision, Recall, f1, support\n",
    "    p, r, f1, s = precision_recall_fscore_support(y_true, y_pred)\n",
    "    # 计算总体的平均Precision, Recall, f1, support\n",
    "    tot_p = np.average(p, weights=s)\n",
    "    tot_r = np.average(r, weights=s)\n",
    "    tot_f1 = np.average(f1, weights=s)\n",
    "    tot_s = np.sum(s)\n",
    "    res1 = pd.DataFrame({\n",
    "        u'Label': labels,\n",
    "        u'Precision': p,\n",
    "        u'Recall': r,\n",
    "        u'F1': f1,\n",
    "        u'Support': s\n",
    "    })\n",
    "    res2 = pd.DataFrame({\n",
    "        u'Label': ['总体'],\n",
    "        u'Precision': [tot_p],\n",
    "        u'Recall': [tot_r],\n",
    "        u'F1': [tot_f1],\n",
    "        u'Support': [tot_s]\n",
    "    })\n",
    "    res2.index = [999]\n",
    "    res = pd.concat([res1, res2])\n",
    "    return res[['Label', 'Precision', 'Recall', 'F1', 'Support']]\n",
    "\n",
    "eval_model(test_label_list, predict_label_list, label.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
